{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Models tuning\n",
    "\n",
    "In previous notebooks, we conducted extensive experiments with numerous models and a wide range of features. In this notebook, we narrow our focus to a single model and a specific subset of features that are available in production. We will perform hyperparameter tuning and feature selection, utilizing techniques like cross-validation to refine the model. Our goal is to identify the most effective feature combinations and hyperparameter settings to optimize the model's accuracy for production deployment.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Import dependencies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "import xgboost as xgb\n",
    "import warnings\n",
    "import os\n",
    "from hyperopt import hp, fmin, tpe, Trials, STATUS_OK\n",
    "from sklearn.metrics import mean_absolute_error, mean_squared_error\n",
    "\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "pd.set_option('display.max_rows', None)\n",
    "pd.set_option('display.max_columns', None)\n",
    "pd.set_option('display.width', 2000)\n",
    "pd.set_option('display.float_format', '{:20,.2f}'.format)\n",
    "pd.set_option('display.max_colwidth', None)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load the training dataset containing only the features to be used in production + engineered features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "GEN_DATA_PATH = os.path.join('..','..', 'data', '5_training_production_model_data.parquet')\n",
    "df = pd.read_parquet(GEN_DATA_PATH)\n",
    "TARGET_COL = 'DC Gen. Power'\n",
    "display(df.head(5))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = df.drop(columns=[TARGET_COL])\n",
    "y = df[TARGET_COL]\n",
    "x_scaler = StandardScaler()\n",
    "y_scaler = StandardScaler()\n",
    "x_scaled = x_scaler.fit_transform(x)\n",
    "y_scaled = y_scaler.fit_transform(y.values.reshape(-1, 1))\n",
    "\n",
    "print('All: ', len(df))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "results = pd.DataFrame()\n",
    "def calculate_metrics(y_pred, y_val, model_name, hyperparams, fold, features):\n",
    "    \"\"\"\n",
    "    Calculate metrics for the model\n",
    "    \"\"\"\n",
    "    global results\n",
    "    mse = mean_squared_error(y_val, y_pred)\n",
    "    rmse = np.sqrt(mse)\n",
    "    mae = mean_absolute_error(y_val, y_pred)    \n",
    "\n",
    "    df = pd.DataFrame({\n",
    "        'model': [model_name],        \n",
    "        'mae': [mae],\n",
    "        'mse': [mse],\n",
    "        'mae': [mae],\n",
    "        'hyperparams': [hyperparams],\n",
    "        'fold': [fold],\n",
    "        'features': [features]\n",
    "    })\n",
    "    results = pd.concat([results, df])   \n",
    "\n",
    "    return mse, rmse, mae\n",
    "\n",
    "def plot_predictions(y_pred, y_actual, n=200):\n",
    "    plt.figure(figsize=(20, 8))\n",
    "    plt.plot(y_actual[-n:], label='Actual')\n",
    "    plt.plot(y_pred[-n:], label='Predicted')\n",
    "    plt.legend()\n",
    "    plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Feature Selection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "essential_features = ['Temperature', 'Precipitation' 'Shortwave Radiation']\n",
    "\n",
    "all_features = x.columns.tolist()\n",
    "\n",
    "temporal_features = [\n",
    "    'day', 'season_0', 'season_1', 'season_2', 'season_3', \n",
    "    'month_1', 'month_2', 'month_3', 'month_4', 'month_5', \n",
    "    'month_6', 'month_7', 'month_8', 'month_9', 'month_10', \n",
    "    'month_11', 'month_12', 'hour_0', 'hour_1', 'hour_2', \n",
    "    'hour_3', 'hour_4', 'hour_5', 'hour_6', 'hour_7', \n",
    "    'hour_8', 'hour_9', 'hour_10', 'hour_11', 'hour_12', \n",
    "    'hour_13', 'hour_14', 'hour_15', 'hour_16', 'hour_17', \n",
    "    'hour_18', 'hour_19', 'hour_20', 'hour_21', 'hour_22', \n",
    "    'hour_23', 'Hours Since Last Rain'\n",
    "]\n",
    "\n",
    "lag_features = [\n",
    "    'DC Gen. Power 1 Hour Lag', 'DC Gen. Power 2 Hour Lag', 'DC Gen. Power 4 Hour Lag', \n",
    "    'DC Gen. Power 24 Hour Lag', 'DC Gen. Power 720 Hour Lag', \n",
    "    'DC Gen. Power 24 Hour Rolling Mean', 'DC Gen. Power 24 Hour Rolling Std', \n",
    "    'DC Gen. Power 24 Hour Rolling Max', 'DC Gen. Power 24 Hour Rolling EMA', \n",
    "    'DC Gen. Power 48 Hour Rolling Mean', 'DC Gen. Power 48 Hour Rolling Std', \n",
    "    'DC Gen. Power 48 Hour Rolling Max', 'DC Gen. Power 48 Hour Rolling EMA', \n",
    "    'DC Gen. Power 720 Hour Rolling Mean', 'DC Gen. Power 720 Hour Rolling Std', \n",
    "    'DC Gen. Power 720 Hour Rolling Max', 'DC Gen. Power 720 Hour Rolling EMA'\n",
    "]\n",
    "\n",
    "\n",
    "rolling_features = [x for x in df.columns if 'Rolling' in x]\n",
    "seasonal_temporal_features = [\n",
    "    'day', 'season_0', 'season_1', 'season_2', 'season_3', \n",
    "    'month_1', 'month_2', 'month_3', 'month_4', 'month_5', \n",
    "    'month_6', 'month_7', 'month_8', 'month_9', 'month_10', \n",
    "    'month_11', 'month_12', 'hour_0', 'hour_1', 'hour_2', \n",
    "    'hour_3', 'hour_4', 'hour_5', 'hour_6', 'hour_7', \n",
    "    'hour_8', 'hour_9', 'hour_10', 'hour_11', 'hour_12', \n",
    "    'hour_13', 'hour_14', 'hour_15', 'hour_16', 'hour_17', \n",
    "    'hour_18', 'hour_19', 'hour_20', 'hour_21', 'hour_22', \n",
    "    'hour_23'\n",
    "]\n",
    "\n",
    "domain_knowledge_features =  ['Hours Since Last Rain','Solar Zenith Angle']\n",
    "\n",
    "FEATURE_SETS = {\n",
    "    'all': all_features,\n",
    "    'temporal': essential_features + temporal_features,\n",
    "    'lag': essential_features + lag_features,\n",
    "    'rolling': essential_features + rolling_features,\n",
    "    'seasonal_temporal': essential_features + seasonal_temporal_features,\n",
    "    'domain_knowledge': essential_features + domain_knowledge_features\n",
    "}\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## K-Fold Cross Validation\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import TimeSeriesSplit\n",
    "\n",
    "tscv = TimeSeriesSplit(n_splits=5, test_size=int(0.12*len(x)))\n",
    "results = pd.DataFrame()\n",
    "\n",
    "def run_k_fold_cv(model_factory, space, max_evals=100):\n",
    "    global best_loss, best_model, y_scaler, results\n",
    " \n",
    "    best_loss = np.inf\n",
    "    best_model = None\n",
    "    \n",
    "    for fold, (train_idx, val_idx) in enumerate(tscv.split(x)):\n",
    "        for features_name, feature_set in FEATURE_SETS.items(): \n",
    "            x_fold = x.drop(columns=[col for col in x.columns if col not in feature_set])\n",
    "            x_fold_scaled = x_scaler.fit_transform(x_fold)\n",
    "            print(f\"Fold {fold + 1}\")\n",
    "            print(f\"Train: {len(train_idx)}\")\n",
    "            print(f\"Validation: {len(val_idx)}\")\n",
    "            x_train_fold, x_val_fold = x_fold_scaled[train_idx], x_fold_scaled[val_idx]\n",
    "            y_train_fold, y_val_fold = y_scaled[train_idx], y[val_idx]\n",
    "            \n",
    "            def objective(params):\n",
    "                model = model_factory(**params)\n",
    "                model.fit(x_train_fold, y_train_fold)\n",
    "                y_pred = model.predict(x_val_fold)\n",
    "                y_pred = y_scaler.inverse_transform(y_pred.reshape(-1, 1))\n",
    "                mae = mean_absolute_error(y_val_fold, y_pred)\n",
    "                model_name = model.__class__.__name__\n",
    "                calculate_metrics(y_pred, y_val_fold, model_name, params, fold +1, features_name)\n",
    "                global best_loss, best_model\n",
    "                if mae < best_loss:\n",
    "                    best_loss = mae\n",
    "                    best_model = model\n",
    "                    \n",
    "                return {'loss': mae, 'status': STATUS_OK}\n",
    "            \n",
    "            trials = Trials()\n",
    "            fmin(objective, space, algo=tpe.suggest, max_evals=max_evals, trials=trials)\n",
    "    \n",
    "    return best_model\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### CatBoost Regressor\n",
    "\n",
    "Given that CatBoost models have delivered the best results in prior experiments, we will focus exclusively on fine-tuning the hyperparameters for this model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from catboost import CatBoostRegressor\n",
    "\n",
    "model_factory = lambda **params: CatBoostRegressor(**params, verbose=False)\n",
    "space = {\n",
    "    'iterations': hp.choice('iterations', [50, 100, 200, 400]),\n",
    "    'learning_rate': hp.uniform('learning_rate', 0.01, 0.2),\n",
    "    'depth': hp.uniformint('depth', 3, 15),\n",
    "    'l2_leaf_reg': hp.uniform('l2_leaf_reg', 1, 10),\n",
    "    'border_count': hp.uniformint('border_count', 32, 255),\n",
    "    'subsample': hp.uniform('subsample', 0.5, 1.0),\n",
    "    'rsm': hp.uniform('rsm', 0.5, 1.0),\n",
    "    'random_strength': hp.uniform('random_strength', 0.5, 1.0),\n",
    "    'bagging_temperature': hp.uniform('bagging_temperature', 0.0, 1.0),\n",
    "}\n",
    "\n",
    "best_model = run_k_fold_cv(model_factory, space, max_evals=100)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "model              features  hyperparams                                                                                                                                                                                                                                                                \n",
       "CatBoostRegressor  all       {'bagging_temperature': 0.3960590286279437, 'border_count': 188, 'depth': 7, 'iterations': 400, 'l2_leaf_reg': 1.327925877118482, 'learning_rate': 0.0772764990599374, 'random_strength': 0.518174597745601, 'rsm': 0.9381818176925532, 'subsample': 0.848270593585808}                   4,109.71\n",
       "                             {'bagging_temperature': 0.5370187694440764, 'border_count': 188, 'depth': 7, 'iterations': 400, 'l2_leaf_reg': 1.0118314912996154, 'learning_rate': 0.07470502426645866, 'random_strength': 0.5236926881366905, 'rsm': 0.7214254522691629, 'subsample': 0.8434584093158926}               4,115.68\n",
       "                             {'bagging_temperature': 0.5638581507504913, 'border_count': 231, 'depth': 6, 'iterations': 400, 'l2_leaf_reg': 1.3432573912250583, 'learning_rate': 0.08386830485328264, 'random_strength': 0.5164659440784284, 'rsm': 0.8208313868136171, 'subsample': 0.7430536289486115}               4,121.47\n",
       "                             {'bagging_temperature': 0.3985093707708194, 'border_count': 170, 'depth': 6, 'iterations': 400, 'l2_leaf_reg': 1.3158572722295605, 'learning_rate': 0.09352212459896268, 'random_strength': 0.5762711141194412, 'rsm': 0.6885742229492033, 'subsample': 0.8996940690144981}               4,177.00\n",
       "                             {'bagging_temperature': 0.9697998352158186, 'border_count': 123, 'depth': 5, 'iterations': 400, 'l2_leaf_reg': 4.18157751432967, 'learning_rate': 0.13321820709044072, 'random_strength': 0.5437030800315786, 'rsm': 0.7108896567884168, 'subsample': 0.5226429662851458}                 4,193.66\n",
       "Name: mae, dtype: float64"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "results['hyperparams'] = results['hyperparams'].astype(str)\n",
    "results_group_all_folds = results.groupby(['model', 'features','hyperparams']).mae.mean().sort_values()\n",
    "results_group_all_folds.head(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the next notebook, we will apply the best hyperparameter configurations and the most effective feature set identified in these experiments to train the final production model."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
