{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Models tuning\n",
    "\n",
    "In this notebook, we will perform hyperparameter tuning and feature selection to optimize the performance of various machine learning models for power generation prediction. \n",
    "We will use techniques like cross-validation and different feature subsets to refine our models. Our goal is to identify the most effective feature combinations and hyperparameter settings to improve model accuracy.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Import dependencies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.linear_model import LinearRegression, LassoCV, RidgeCV, ElasticNet\n",
    "from sklearn.ensemble import GradientBoostingRegressor, RandomForestRegressor\n",
    "from sklearn.tree import DecisionTreeRegressor\n",
    "from sklearn.inspection import permutation_importance\n",
    "from sklearn.pipeline import make_pipeline\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "import xgboost as xgb\n",
    "from functools import partial\n",
    "import warnings\n",
    "import os\n",
    "from hyperopt import hp, fmin, tpe, Trials, STATUS_OK\n",
    "from sklearn.metrics import mean_absolute_error, mean_squared_error\n",
    "\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "pd.set_option('display.max_rows', None)\n",
    "pd.set_option('display.max_columns', None)\n",
    "pd.set_option('display.width', 2000)\n",
    "pd.set_option('display.float_format', '{:20,.2f}'.format)\n",
    "pd.set_option('display.max_colwidth', None)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load the historical generation and weather training data that was augmented using feature engineering in the previous notebook"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "PRE_PROCESSED_DATA_PATH = os.path.join('..','..', 'data', '1_pre_processed_data.parquet')\n",
    "GEN_DATA_PATH = os.path.join('..','..', 'data', '2_feature_engineered_data.parquet')\n",
    "df = pd.read_parquet(GEN_DATA_PATH)\n",
    "df_without_feature_engineering = pd.read_parquet(PRE_PROCESSED_DATA_PATH)\n",
    "TARGET_COL = 'DC Gen. Power'\n",
    "display(df.head(5))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = df.drop(columns=[TARGET_COL])\n",
    "y = df[TARGET_COL]\n",
    "x_scaler = StandardScaler()\n",
    "y_scaler = StandardScaler()\n",
    "x_scaled = x_scaler.fit_transform(x)\n",
    "y_scaled = y_scaler.fit_transform(y.values.reshape(-1, 1))\n",
    "\n",
    "print('All: ', len(df))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Training\n",
    "\n",
    "This section focuses on training machine learning models for power generation prediction. We will explore various algorithms, optimize their hyperparameters, and evaluate their performance to identify the best model for this task."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "results = pd.DataFrame()\n",
    "def calculate_metrics(y_pred, y_val, model_name, hyperparams, fold, features):\n",
    "    \"\"\"\n",
    "    Calculate metrics for the model\n",
    "    \"\"\"\n",
    "    global results\n",
    "    mse = mean_squared_error(y_val, y_pred)\n",
    "    rmse = np.sqrt(mse)\n",
    "    mae = mean_absolute_error(y_val, y_pred)    \n",
    "\n",
    "    df = pd.DataFrame({\n",
    "        'model': [model_name],        \n",
    "        'mae': [mae],\n",
    "        'mse': [mse],\n",
    "        'mae': [mae],\n",
    "        'hyperparams': [hyperparams],\n",
    "        'fold': [fold],\n",
    "        'features': [features]\n",
    "    })\n",
    "    results = pd.concat([results, df])   \n",
    "\n",
    "    return mse, rmse, mae\n",
    "\n",
    "def plot_predictions(y_pred, y_actual, n=200):\n",
    "    plt.figure(figsize=(20, 8))\n",
    "    plt.plot(y_actual[-n:], label='Actual')\n",
    "    plt.plot(y_pred[-n:], label='Predicted')\n",
    "    plt.legend()\n",
    "    plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Feature Selection\n",
    "\n",
    "Feature selection is a crucial step in the modeling process that involves choosing the most relevant features from the dataset to improve model performance and reduce complexity. By selecting a subset of features, we aim to enhance the model's ability to generalize, reduce overfitting, and expedite the training process.\n",
    "\n",
    "In this notebook, we experiment with various subsets of features to evaluate their impact on model performance. We will explore different feature sets, such as all available features, basic features, important features identified through domain knowledge, and specific subsets excluding certain types of features like lags or rolling metrics. This approach allows us to identify which features contribute most significantly to the predictive power of the model and optimize feature selection for better results.\n",
    "\n",
    "We will employ K-Fold Cross Validation to assess the effectiveness of these feature subsets and determine how they influence model accuracy and robustness across different machine learning algorithms.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "all_features = x.columns.tolist()\n",
    "\n",
    "FEATURE_SETS = {\n",
    "    'all': essential_features + all_features,\n",
    "\n",
    "}\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## K-Fold Cross Validation\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this section, we perform K-Fold Cross Validation to evaluate and optimize the performance of various machine learning models. K-Fold Cross Validation is a technique that involves partitioning the dataset into \\( k \\) distinct folds, where each fold is used once as a validation set while the remaining \\( k-1 \\) folds are used for training. This process is repeated \\( k \\) times, ensuring that each data point gets to be in a validation set exactly once. This approach helps in assessing the model's ability to generalize to new, unseen data and in tuning hyperparameters to enhance model performance.\n",
    "\n",
    "We will use this method to test a range of models, our objective is to find the most effective model and hyperparameter settings by comparing their performance across different feature subsets and folds.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import TimeSeriesSplit\n",
    "\n",
    "tscv = TimeSeriesSplit(n_splits=5, test_size=int(0.12*len(x)))\n",
    "results = pd.DataFrame()\n",
    "\n",
    "def run_k_fold_cv(model_factory, space, max_evals=100):\n",
    "    global best_loss, best_model, y_scaler, results\n",
    " \n",
    "    best_loss = np.inf\n",
    "    best_model = None\n",
    "    \n",
    "    for fold, (train_idx, val_idx) in enumerate(tscv.split(x)):\n",
    "        for features_name, feature_set in FEATURE_SETS.items(): \n",
    "            x_fold = x.drop(columns=[col for col in x.columns if col not in feature_set])\n",
    "            x_fold_scaled = x_scaler.fit_transform(x_fold)\n",
    "            print(f\"Fold {fold + 1}\")\n",
    "            print(f\"Train: {len(train_idx)}\")\n",
    "            print(f\"Validation: {len(val_idx)}\")\n",
    "            x_train_fold, x_val_fold = x_fold_scaled[train_idx], x_fold_scaled[val_idx]\n",
    "            y_train_fold, y_val_fold = y_scaled[train_idx], y[val_idx]\n",
    "            \n",
    "            def objective(params):\n",
    "                model = model_factory(**params)\n",
    "                model.fit(x_train_fold, y_train_fold)\n",
    "                y_pred = model.predict(x_val_fold)\n",
    "                y_pred = y_scaler.inverse_transform(y_pred.reshape(-1, 1))\n",
    "                mae = mean_absolute_error(y_val_fold, y_pred)\n",
    "                model_name = model.__class__.__name__\n",
    "                calculate_metrics(y_pred, y_val_fold, model_name, params, fold +1, features_name)\n",
    "                global best_loss, best_model\n",
    "                if mae < best_loss:\n",
    "                    best_loss = mae\n",
    "                    best_model = model\n",
    "                    \n",
    "                return {'loss': mae, 'status': STATUS_OK}\n",
    "            \n",
    "            trials = Trials()\n",
    "            fmin(objective, space, algo=tpe.suggest, max_evals=max_evals, trials=trials)\n",
    "    \n",
    "    return best_model\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Linear models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_factory = lambda **params: LinearRegression(**params)\n",
    "\n",
    "space = {\n",
    "    'fit_intercept': hp.choice('fit_intercept', [True]),\n",
    "}\n",
    "\n",
    "run_k_fold_cv(model_factory, space, max_evals=2)\n",
    "# y_pred = best_model.predict(x_scaled)\n",
    "# y_pred = y_scaler.inverse_transform(y_pred.reshape(-1, 1))\n",
    "# plot_predictions(y_pred, y.values, 1000)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ElasticNet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_factory = lambda **params: ElasticNet(**params)\n",
    "space = {\n",
    "    'alpha': hp.uniform('alpha', 0.01, 1.0),\n",
    "    'l1_ratio': hp.uniform('l1_ratio', 0.01, 1.0),\n",
    "    'fit_intercept': hp.choice('fit_intercept', [True]),\n",
    "    'tol': hp.uniform('tol', 0.00001, 0.1),\n",
    "    'selection': hp.choice('selection', ['cyclic']),\n",
    "    'max_iter': hp.choice('max_iter', [100, 200, 400, 1000 ]),\n",
    "}\n",
    "\n",
    "best_model = run_k_fold_cv(model_factory, space, max_evals=10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Decision trees\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Random Forest Regressors\n",
    "Random Forests (RFs) leverage ensemble learning, a technique that combines predictions from multiple decision tree regressors (DTRs) to achieve improved model robustness and potentially superior prediction accuracy."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_factory = lambda **params: RandomForestRegressor(**params)\n",
    "\n",
    "space = {\n",
    "    'n_estimators': hp.choice('n_estimators', [10, 20, 40]),\n",
    "    'max_depth': hp.choice('max_depth', [None, 4, 8, 16, 32, 64, 128]),\n",
    "    'min_samples_split': hp.choice('min_samples_split', [10, 20, 40]),\n",
    "    'min_samples_leaf': hp.choice('min_samples_leaf', [2, 4, 8]),\n",
    "    'max_features': hp.choice('max_features', [ 8, 16, 32, 64]),\n",
    "}\n",
    "\n",
    "best_model = run_k_fold_cv(model_factory, space, max_evals=5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Gradient Boost Machine"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_factory = lambda **params: xgb.XGBRegressor(**params, n_jobs=-1)\n",
    "space = {\n",
    "    'n_estimators': hp.choice('n_estimators', [50]),\n",
    "    'learning_rate': hp.uniform('learning_rate', 0.01, 0.2),\n",
    "    'max_depth': hp.uniformint('max_depth', 3, 64),\n",
    "    'min_child_weight': hp.uniformint('min_child_weight', 1, 8),\n",
    "    'subsample': hp.uniform('subsample', 0.5, 1.0),\n",
    "    'colsample_bytree': hp.uniform('colsample_bytree', 0.5, 1.0),\n",
    "    'colsample_bylevel': hp.uniform('colsample_bylevel', 0.5, 1.0),\n",
    "    'colsample_bynode': hp.uniform('colsample_bynode', 0.5, 1.0),\n",
    "    'reg_alpha': hp.uniform('reg_alpha', 0.0, 1.0),\n",
    "    'reg_lambda': hp.uniform('reg_lambda', 0.0, 1.0),\n",
    "}\n",
    "\n",
    "best_model = run_k_fold_cv(model_factory, space, max_evals=10)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### CatBoost Regressor\n",
    "\n",
    "CatBoost is a powerful gradient boosting library that is optimized for categorical features and robust to various data types. Developed by Yandex, CatBoost excels in handling complex datasets with categorical variables and provides high-performance predictive modeling. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from catboost import CatBoostRegressor\n",
    "\n",
    "model_factory = lambda **params: CatBoostRegressor(**params, verbose=False)\n",
    "space = {\n",
    "    'iterations': hp.choice('iterations', [50, 100, 200, 400]),\n",
    "    'learning_rate': hp.uniform('learning_rate', 0.01, 0.2),\n",
    "    'depth': hp.uniformint('depth', 3, 10),\n",
    "    'l2_leaf_reg': hp.uniform('l2_leaf_reg', 1, 10),\n",
    "    'border_count': hp.uniformint('border_count', 32, 255),\n",
    "}\n",
    "\n",
    "best_model = run_k_fold_cv(model_factory, space, max_evals=10)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Neural Networks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# MLPRegressor\n",
    "from sklearn.neural_network import MLPRegressor\n",
    "model_factory = lambda **params: MLPRegressor(**params)\n",
    "space = {\n",
    "    'hidden_layer_sizes': hp.choice('hidden_layer_sizes', [(16,), (16, 16), (16, 16, 16), (16, 16, 16, 16), (32,), (32, 32), (64,), (64, 64)]),\n",
    "    'activation': hp.choice('activation', ['relu']),\n",
    "    'solver': hp.choice('solver', ['adam']),\n",
    "    'alpha': hp.uniform('alpha', 0.0001, 0.1),\n",
    "    'batch_size': hp.choice('batch_size', [32, 64, 128]),\n",
    "    'learning_rate': hp.choice('learning_rate', ['constant', 'invscaling', 'adaptive']),\n",
    "    'learning_rate_init': hp.uniform('learning_rate_init', 0.0001, 0.1),\n",
    "    'max_iter': hp.choice('max_iter', [10000]),\n",
    "    'early_stopping': hp.choice('early_stopping', [True]),\n",
    "    'n_iter_no_change': hp.choice('n_iter_no_change', [1, 5, 10]),\n",
    "}\n",
    "\n",
    "best_model = run_k_fold_cv(model_factory, space, max_evals=10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### LightGBM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_actual = y_scaler.inverse_transform(y_val_cnn)\n",
    "mae_cnn = mean_absolute_error(y_actual, y_pred_cnn)\n",
    "mae_cnn"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "display(results.sort_values('mae').head(20))\n",
    "results_group_all_folds = results.groupby(['model', 'features']).mae.mean().sort_values()\n",
    "results_grouped = results_group_all_folds.groupby(['model', 'features']).min().sort_values()\n",
    "\n",
    "plt.figure(figsize=(20, 8))\n",
    "results_grouped.plot(kind='bar')\n",
    "\n",
    "plt.title('Model Performance')\n",
    "plt.ylabel('MAE')\n",
    "plt.xlabel('Model')\n",
    "plt.xticks(rotation=45)\n",
    "\n",
    "plt.show()\n",
    "display(results_grouped)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "results_group_all_folds"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
