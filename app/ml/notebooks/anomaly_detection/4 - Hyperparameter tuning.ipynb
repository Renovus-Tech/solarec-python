{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Models tuning\n",
    "\n",
    "In this notebook, we will perform hyperparameter tuning and feature selection to optimize the performance of various machine learning models for power generation prediction. \n",
    "We will use techniques like cross-validation and different feature subsets to refine our models. Our goal is to identify the most effective feature combinations and hyperparameter settings to improve model accuracy.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Import dependencies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.linear_model import LinearRegression, LassoCV, RidgeCV, ElasticNet\n",
    "from sklearn.ensemble import GradientBoostingRegressor, RandomForestRegressor\n",
    "from sklearn.tree import DecisionTreeRegressor\n",
    "from sklearn.inspection import permutation_importance\n",
    "from sklearn.pipeline import make_pipeline\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "import xgboost as xgb\n",
    "from functools import partial\n",
    "import warnings\n",
    "import os\n",
    "from hyperopt import hp, fmin, tpe, Trials, STATUS_OK\n",
    "from sklearn.metrics import mean_absolute_error, mean_squared_error\n",
    "\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "pd.set_option('display.max_rows', None)\n",
    "pd.set_option('display.max_columns', None)\n",
    "pd.set_option('display.width', 2000)\n",
    "pd.set_option('display.float_format', '{:20,.2f}'.format)\n",
    "pd.set_option('display.max_colwidth', None)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load the historical generation and weather training data that was augmented using feature engineering in the previous notebook"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "PRE_PROCESSED_DATA_PATH = os.path.join('..','..', 'data', '1_pre_processed_data.parquet')\n",
    "GEN_DATA_PATH = os.path.join('..','..', 'data', '2_feature_engineered_data.parquet')\n",
    "df = pd.read_parquet(GEN_DATA_PATH)\n",
    "df_without_feature_engineering = pd.read_parquet(PRE_PROCESSED_DATA_PATH)\n",
    "TARGET_COL = 'DC Gen. Power'\n",
    "display(df.head(5))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = df.drop(columns=[TARGET_COL])\n",
    "y = df[TARGET_COL]\n",
    "x_scaler = StandardScaler()\n",
    "y_scaler = StandardScaler()\n",
    "x_scaled = x_scaler.fit_transform(x)\n",
    "y_scaled = y_scaler.fit_transform(y.values.reshape(-1, 1))\n",
    "\n",
    "print('All: ', len(df))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Training\n",
    "\n",
    "This section focuses on training machine learning models for power generation prediction. We will explore various algorithms, optimize their hyperparameters, and evaluate their performance to identify the best model for this task."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "results = pd.DataFrame()\n",
    "def calculate_metrics(y_pred, y_val, model_name, hyperparams, fold, features):\n",
    "    \"\"\"\n",
    "    Calculate metrics for the model\n",
    "    \"\"\"\n",
    "    global results\n",
    "    mse = mean_squared_error(y_val, y_pred)\n",
    "    rmse = np.sqrt(mse)\n",
    "    mae = mean_absolute_error(y_val, y_pred)    \n",
    "\n",
    "    df = pd.DataFrame({\n",
    "        'model': [model_name],        \n",
    "        'mae': [mae],\n",
    "        'mse': [mse],\n",
    "        'mae': [mae],\n",
    "        'hyperparams': [hyperparams],\n",
    "        'fold': [fold],\n",
    "        'features': [features]\n",
    "    })\n",
    "    results = pd.concat([results, df])   \n",
    "\n",
    "    return mse, rmse, mae\n",
    "\n",
    "def plot_predictions(y_pred, y_actual, n=200):\n",
    "    plt.figure(figsize=(20, 8))\n",
    "    plt.plot(y_actual[-n:], label='Actual')\n",
    "    plt.plot(y_pred[-n:], label='Predicted')\n",
    "    plt.legend()\n",
    "    plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Feature Selection\n",
    "\n",
    "Feature selection is a crucial step in the modeling process that involves choosing the most relevant features from the dataset to improve model performance and reduce complexity. By selecting a subset of features, we aim to enhance the model's ability to generalize, reduce overfitting, and expedite the training process.\n",
    "\n",
    "In this notebook, we experiment with various subsets of features to evaluate their impact on model performance. We will explore different feature sets, such as all available features, basic features, important features identified through domain knowledge, and specific subsets excluding certain types of features like lags or rolling metrics. This approach allows us to identify which features contribute most significantly to the predictive power of the model and optimize feature selection for better results.\n",
    "\n",
    "We will employ K-Fold Cross Validation to assess the effectiveness of these feature subsets and determine how they influence model accuracy and robustness across different machine learning algorithms.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "essential_features = [\n",
    "    'Temperature', 'Shortwave Radiation', 'Longwave Radiation', 'UV Radiation', \n",
    "    'Direct Shortwave Radiation', 'Diffuse Shortwave Radiation'\n",
    "]\n",
    "\n",
    "all_features = x.columns.tolist()\n",
    "\n",
    "basic_features = df_without_feature_engineering.columns.tolist()\n",
    "\n",
    "temporal_features = [\n",
    "    'day', 'season_0', 'season_1', 'season_2', 'season_3', \n",
    "    'month_1', 'month_2', 'month_3', 'month_4', 'month_5', \n",
    "    'month_6', 'month_7', 'month_8', 'month_9', 'month_10', \n",
    "    'month_11', 'month_12', 'hour_0', 'hour_1', 'hour_2', \n",
    "    'hour_3', 'hour_4', 'hour_5', 'hour_6', 'hour_7', \n",
    "    'hour_8', 'hour_9', 'hour_10', 'hour_11', 'hour_12', \n",
    "    'hour_13', 'hour_14', 'hour_15', 'hour_16', 'hour_17', \n",
    "    'hour_18', 'hour_19', 'hour_20', 'hour_21', 'hour_22', \n",
    "    'hour_23', 'Hours Since Last Rain'\n",
    "]\n",
    "\n",
    "lag_features = [\n",
    "    'DC Gen. Power 1 Hour Lag', 'DC Gen. Power 2 Hour Lag', 'DC Gen. Power 4 Hour Lag', \n",
    "    'DC Gen. Power 24 Hour Lag', 'DC Gen. Power 720 Hour Lag', \n",
    "    'DC Gen. Power 24 Hour Rolling Mean', 'DC Gen. Power 24 Hour Rolling Std', \n",
    "    'DC Gen. Power 24 Hour Rolling Max', 'DC Gen. Power 24 Hour Rolling EMA', \n",
    "    'DC Gen. Power 48 Hour Rolling Mean', 'DC Gen. Power 48 Hour Rolling Std', \n",
    "    'DC Gen. Power 48 Hour Rolling Max', 'DC Gen. Power 48 Hour Rolling EMA', \n",
    "    'DC Gen. Power 720 Hour Rolling Mean', 'DC Gen. Power 720 Hour Rolling Std', \n",
    "    'DC Gen. Power 720 Hour Rolling Max', 'DC Gen. Power 720 Hour Rolling EMA'\n",
    "]\n",
    "\n",
    "rolling_features = [\n",
    "    'Shortwave Radiation 24 Hour Rolling Mean', 'Shortwave Radiation 24 Hour Rolling Std', \n",
    "    'Shortwave Radiation 24 Hour Rolling Max', 'Shortwave Radiation 24 Hour Rolling EMA', \n",
    "    'Wind Speed 24 Hour Rolling Mean', 'Wind Speed 24 Hour Rolling Std', \n",
    "    'Wind Speed 24 Hour Rolling Max', 'Wind Speed 24 Hour Rolling EMA', \n",
    "    'Temperature 24 Hour Rolling Mean', 'Temperature 24 Hour Rolling Std', \n",
    "    'Temperature 24 Hour Rolling Max', 'Temperature 24 Hour Rolling EMA', \n",
    "    'Relative Humidity 24 Hour Rolling Mean', 'Relative Humidity 24 Hour Rolling Std', \n",
    "    'Relative Humidity 24 Hour Rolling Max', 'Relative Humidity 24 Hour Rolling EMA', \n",
    "    'Shortwave Radiation 48 Hour Rolling Mean', 'Shortwave Radiation 48 Hour Rolling Std', \n",
    "    'Shortwave Radiation 48 Hour Rolling Max', 'Shortwave Radiation 48 Hour Rolling EMA', \n",
    "    'Wind Speed 48 Hour Rolling Mean', 'Wind Speed 48 Hour Rolling Std', \n",
    "    'Wind Speed 48 Hour Rolling Max', 'Wind Speed 48 Hour Rolling EMA', \n",
    "    'Temperature 48 Hour Rolling Mean', 'Temperature 48 Hour Rolling Std', \n",
    "    'Temperature 48 Hour Rolling Max', 'Temperature 48 Hour Rolling EMA', \n",
    "    'Relative Humidity 48 Hour Rolling Mean', 'Relative Humidity 48 Hour Rolling Std', \n",
    "    'Relative Humidity 48 Hour Rolling Max', 'Relative Humidity 48 Hour Rolling EMA', \n",
    "    'Shortwave Radiation 720 Hour Rolling Mean', 'Shortwave Radiation 720 Hour Rolling Std', \n",
    "    'Shortwave Radiation 720 Hour Rolling Max', 'Shortwave Radiation 720 Hour Rolling EMA', \n",
    "    'Wind Speed 720 Hour Rolling Mean', 'Wind Speed 720 Hour Rolling Std', \n",
    "    'Wind Speed 720 Hour Rolling Max', 'Wind Speed 720 Hour Rolling EMA', \n",
    "    'Temperature 720 Hour Rolling Mean', 'Temperature 720 Hour Rolling Std', \n",
    "    'Temperature 720 Hour Rolling Max', 'Temperature 720 Hour Rolling EMA', \n",
    "    'Relative Humidity 720 Hour Rolling Mean', 'Relative Humidity 720 Hour Rolling Std', \n",
    "    'Relative Humidity 720 Hour Rolling Max', 'Relative Humidity 720 Hour Rolling EMA'\n",
    "]\n",
    "\n",
    "seasonal_temporal_features = [\n",
    "    'day', 'season_0', 'season_1', 'season_2', 'season_3', \n",
    "    'month_1', 'month_2', 'month_3', 'month_4', 'month_5', \n",
    "    'month_6', 'month_7', 'month_8', 'month_9', 'month_10', \n",
    "    'month_11', 'month_12', 'hour_0', 'hour_1', 'hour_2', \n",
    "    'hour_3', 'hour_4', 'hour_5', 'hour_6', 'hour_7', \n",
    "    'hour_8', 'hour_9', 'hour_10', 'hour_11', 'hour_12', \n",
    "    'hour_13', 'hour_14', 'hour_15', 'hour_16', 'hour_17', \n",
    "    'hour_18', 'hour_19', 'hour_20', 'hour_21', 'hour_22', \n",
    "    'hour_23'\n",
    "]\n",
    "\n",
    "domain_knowledge_features =  ['Hours Since Last Rain','days_since_installation','Wind Chill','Solar Zenith Angle']\n",
    "\n",
    "FEATURE_SETS = {\n",
    "    'all': all_features,\n",
    "    'basic': essential_features + basic_features,\n",
    "    'temporal': essential_features + temporal_features,\n",
    "    'lag': essential_features + lag_features,\n",
    "    'rolling': essential_features + rolling_features,\n",
    "    'seasonal_temporal': essential_features + seasonal_temporal_features,\n",
    "    'domain_knowledge': essential_features + domain_knowledge_features\n",
    "}\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## K-Fold Cross Validation\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this section, we perform K-Fold Cross Validation to evaluate and optimize the performance of various machine learning models. K-Fold Cross Validation is a technique that involves partitioning the dataset into \\( k \\) distinct folds, where each fold is used once as a validation set while the remaining \\( k-1 \\) folds are used for training. This process is repeated \\( k \\) times, ensuring that each data point gets to be in a validation set exactly once. This approach helps in assessing the model's ability to generalize to new, unseen data and in tuning hyperparameters to enhance model performance.\n",
    "\n",
    "We will use this method to test a range of models, our objective is to find the most effective model and hyperparameter settings by comparing their performance across different feature subsets and folds.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import TimeSeriesSplit\n",
    "\n",
    "tscv = TimeSeriesSplit(n_splits=5, test_size=int(0.12*len(x)))\n",
    "results = pd.DataFrame()\n",
    "\n",
    "def run_k_fold_cv(model_factory, space, max_evals=100):\n",
    "    global best_loss, best_model, y_scaler, results\n",
    " \n",
    "    best_loss = np.inf\n",
    "    best_model = None\n",
    "    \n",
    "    for fold, (train_idx, val_idx) in enumerate(tscv.split(x)):\n",
    "        for features_name, feature_set in FEATURE_SETS.items(): \n",
    "            x_fold = x.drop(columns=[col for col in x.columns if col not in feature_set])\n",
    "            x_fold_scaled = x_scaler.fit_transform(x_fold)\n",
    "            print(f\"Fold {fold + 1}\")\n",
    "            print(f\"Train: {len(train_idx)}\")\n",
    "            print(f\"Validation: {len(val_idx)}\")\n",
    "            x_train_fold, x_val_fold = x_fold_scaled[train_idx], x_fold_scaled[val_idx]\n",
    "            y_train_fold, y_val_fold = y_scaled[train_idx], y[val_idx]\n",
    "            \n",
    "            def objective(params):\n",
    "                model = model_factory(**params)\n",
    "                model.fit(x_train_fold, y_train_fold)\n",
    "                y_pred = model.predict(x_val_fold)\n",
    "                y_pred = y_scaler.inverse_transform(y_pred.reshape(-1, 1))\n",
    "                mae = mean_absolute_error(y_val_fold, y_pred)\n",
    "                model_name = model.__class__.__name__\n",
    "                calculate_metrics(y_pred, y_val_fold, model_name, params, fold +1, features_name)\n",
    "                global best_loss, best_model\n",
    "                if mae < best_loss:\n",
    "                    best_loss = mae\n",
    "                    best_model = model\n",
    "                    \n",
    "                return {'loss': mae, 'status': STATUS_OK}\n",
    "            \n",
    "            trials = Trials()\n",
    "            fmin(objective, space, algo=tpe.suggest, max_evals=max_evals, trials=trials)\n",
    "    \n",
    "    return best_model\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Linear models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_factory = lambda **params: LinearRegression(**params)\n",
    "\n",
    "space = {\n",
    "    'fit_intercept': hp.choice('fit_intercept', [True]),\n",
    "}\n",
    "\n",
    "run_k_fold_cv(model_factory, space, max_evals=2)\n",
    "# y_pred = best_model.predict(x_scaled)\n",
    "# y_pred = y_scaler.inverse_transform(y_pred.reshape(-1, 1))\n",
    "# plot_predictions(y_pred, y.values, 1000)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ElasticNet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_factory = lambda **params: ElasticNet(**params)\n",
    "space = {\n",
    "    'alpha': hp.uniform('alpha', 0.01, 1.0),\n",
    "    'l1_ratio': hp.uniform('l1_ratio', 0.01, 1.0),\n",
    "    'fit_intercept': hp.choice('fit_intercept', [True]),\n",
    "    'tol': hp.uniform('tol', 0.00001, 0.1),\n",
    "    'selection': hp.choice('selection', ['cyclic']),\n",
    "    'max_iter': hp.choice('max_iter', [100, 200, 400, 1000 ]),\n",
    "}\n",
    "\n",
    "best_model = run_k_fold_cv(model_factory, space, max_evals=10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Decision trees\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Random Forest Regressors\n",
    "Random Forests (RFs) leverage ensemble learning, a technique that combines predictions from multiple decision tree regressors (DTRs) to achieve improved model robustness and potentially superior prediction accuracy."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_factory = lambda **params: RandomForestRegressor(**params)\n",
    "\n",
    "space = {\n",
    "    'n_estimators': hp.choice('n_estimators', [10, 20, 40]),\n",
    "    'max_depth': hp.choice('max_depth', [None, 4, 8, 16, 32, 64, 128]),\n",
    "    'min_samples_split': hp.choice('min_samples_split', [10, 20, 40]),\n",
    "    'min_samples_leaf': hp.choice('min_samples_leaf', [2, 4, 8]),\n",
    "    'max_features': hp.choice('max_features', [ 8, 16, 32, 64]),\n",
    "}\n",
    "\n",
    "best_model = run_k_fold_cv(model_factory, space, max_evals=5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Gradient Boost Machine"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_factory = lambda **params: xgb.XGBRegressor(**params, n_jobs=-1)\n",
    "space = {\n",
    "    'n_estimators': hp.choice('n_estimators', [50]),\n",
    "    'learning_rate': hp.uniform('learning_rate', 0.01, 0.2),\n",
    "    'max_depth': hp.uniformint('max_depth', 3, 64),\n",
    "    'min_child_weight': hp.uniformint('min_child_weight', 1, 8),\n",
    "    'subsample': hp.uniform('subsample', 0.5, 1.0),\n",
    "    'colsample_bytree': hp.uniform('colsample_bytree', 0.5, 1.0),\n",
    "    'colsample_bylevel': hp.uniform('colsample_bylevel', 0.5, 1.0),\n",
    "    'colsample_bynode': hp.uniform('colsample_bynode', 0.5, 1.0),\n",
    "    'reg_alpha': hp.uniform('reg_alpha', 0.0, 1.0),\n",
    "    'reg_lambda': hp.uniform('reg_lambda', 0.0, 1.0),\n",
    "}\n",
    "\n",
    "best_model = run_k_fold_cv(model_factory, space, max_evals=10)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### CatBoost Regressor\n",
    "\n",
    "CatBoost is a powerful gradient boosting library that is optimized for categorical features and robust to various data types. Developed by Yandex, CatBoost excels in handling complex datasets with categorical variables and provides high-performance predictive modeling. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from catboost import CatBoostRegressor\n",
    "\n",
    "model_factory = lambda **params: CatBoostRegressor(**params, verbose=False)\n",
    "space = {\n",
    "    'iterations': hp.choice('iterations', [50, 100, 200, 400]),\n",
    "    'learning_rate': hp.uniform('learning_rate', 0.01, 0.2),\n",
    "    'depth': hp.uniformint('depth', 3, 10),\n",
    "    'l2_leaf_reg': hp.uniform('l2_leaf_reg', 1, 10),\n",
    "    'border_count': hp.uniformint('border_count', 32, 255),\n",
    "}\n",
    "\n",
    "best_model = run_k_fold_cv(model_factory, space, max_evals=10)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Neural Networks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# MLPRegressor\n",
    "from sklearn.neural_network import MLPRegressor\n",
    "model_factory = lambda **params: MLPRegressor(**params)\n",
    "space = {\n",
    "    'hidden_layer_sizes': hp.choice('hidden_layer_sizes', [(16,), (16, 16), (16, 16, 16), (16, 16, 16, 16), (32,), (32, 32), (64,), (64, 64)]),\n",
    "    'activation': hp.choice('activation', ['relu']),\n",
    "    'solver': hp.choice('solver', ['adam']),\n",
    "    'alpha': hp.uniform('alpha', 0.0001, 0.1),\n",
    "    'batch_size': hp.choice('batch_size', [32, 64, 128]),\n",
    "    'learning_rate': hp.choice('learning_rate', ['constant', 'invscaling', 'adaptive']),\n",
    "    'learning_rate_init': hp.uniform('learning_rate_init', 0.0001, 0.1),\n",
    "    'max_iter': hp.choice('max_iter', [10000]),\n",
    "    'early_stopping': hp.choice('early_stopping', [True]),\n",
    "    'n_iter_no_change': hp.choice('n_iter_no_change', [1, 5, 10]),\n",
    "}\n",
    "\n",
    "best_model = run_k_fold_cv(model_factory, space, max_evals=10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "display(results.sort_values('mae').head(20))\n",
    "results_group_all_folds = results.groupby(['model', 'features']).mae.mean().sort_values()\n",
    "results_grouped = results_group_all_folds.groupby(['model', 'features']).min().sort_values()\n",
    "\n",
    "plt.figure(figsize=(20, 8))\n",
    "results_grouped.plot(kind='bar')\n",
    "\n",
    "plt.title('Model Performance')\n",
    "plt.ylabel('MAE')\n",
    "plt.xlabel('Model')\n",
    "plt.xticks(rotation=45)\n",
    "\n",
    "plt.show()\n",
    "display(results_grouped)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "model                  features  \n",
       "CatBoostRegressor      no_rolling               4,747.36\n",
       "                       no_lag                   4,958.03\n",
       "RandomForestRegressor  important                5,092.13\n",
       "                       no_rolling               5,169.69\n",
       "                       no_hour                  5,261.82\n",
       "CatBoostRegressor      all                      5,271.81\n",
       "XGBRegressor           important                5,329.35\n",
       "                       all                      5,616.61\n",
       "                       no_rolling               5,620.90\n",
       "RandomForestRegressor  all                      5,669.29\n",
       "CatBoostRegressor      no_hour                  5,669.87\n",
       "XGBRegressor           no_hour                  5,691.27\n",
       "RandomForestRegressor  basic                    5,973.93\n",
       "                       no_lag                   6,028.52\n",
       "MLPRegressor           no_hour                  6,038.15\n",
       "                       important                6,053.46\n",
       "XGBRegressor           no_lag                   6,421.69\n",
       "MLPRegressor           no_rolling               6,448.32\n",
       "CatBoostRegressor      important                6,591.00\n",
       "                       basic                    6,814.66\n",
       "MLPRegressor           all                      7,008.95\n",
       "                       no_lag                   7,380.05\n",
       "LinearRegression       all                      7,558.38\n",
       "XGBRegressor           basic                    7,631.05\n",
       "LinearRegression       no_hour                  7,833.51\n",
       "                       no_rolling               7,901.74\n",
       "MLPRegressor           basic                    7,962.04\n",
       "LinearRegression       important                8,661.84\n",
       "                       no_lag                   9,298.02\n",
       "                       basic                   12,905.53\n",
       "ElasticNet             all                     16,877.82\n",
       "                       no_rolling              16,908.65\n",
       "                       no_hour                 17,161.56\n",
       "                       important               17,301.32\n",
       "                       basic                   17,999.25\n",
       "                       no_lag                  19,328.96\n",
       "Name: mae, dtype: float64"
      ]
     },
     "execution_count": 57,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "results_group_all_folds"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
